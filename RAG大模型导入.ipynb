{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e1c506e",
   "metadata": {},
   "source": [
    "# 基于 LangChain 导入模型的方法\n",
    "- API 导入，我觉得有点麻烦，好处是不用等待下载\n",
    "  - 注意要申请KEY\n",
    "- 私有化本地导入\n",
    "  - 可以进阶做微调等操作\n",
    "  - 要先下载好模型，可以离线下载也可以通过HaggingFace\\Ollama\\Modelscope等下载\n",
    "\n",
    "\n",
    "在官网有一个表，‘Local’列表示是否支持运行本地大语言模型：https://python.langchain.com/docs/integrations/chat/\n",
    "\n",
    "此处选用本地加载"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f01319",
   "metadata": {},
   "source": [
    "# 下载Ollama服务\n",
    "\n",
    "要先安装Ollama服务，对于服务器没有sodu权限的情况看这篇：https://tencentcloud.csdn.net/67875955edd0904849a52d84.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acead177",
   "metadata": {},
   "source": [
    "```js\n",
    "// 下载安装包\n",
    "wget -O ollama-linux-amd64.tgz https://ollama.com/download/ollama-linux-amd64.tgz\n",
    "// 解压缩\n",
    "tar -xvzf ollama-linux-amd64.tgz\n",
    "```\n",
    "然后会出现两个文件夹：`bin`和`lib` \n",
    "\n",
    "如果在没有设置环境变量的情况下，必须在bin文件夹下启动\n",
    "```js\n",
    "cd bin\n",
    "#启动ollama，必须用./xxx\n",
    "./ollama serve\n",
    "```\n",
    "再新建一个终端,进入bin文件夹，运行以下命令查看ollama的版本\n",
    "```\n",
    "#查看ollama版本\n",
    "./ollama -v\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3d618f",
   "metadata": {},
   "source": [
    "设置环境变量\n",
    "\n",
    "启动ollama,设置环境变量后，就可以在任意一个文件夹下启动ollama了。因为我用的实验室的服务器，还有很多同学也在用，所以采用了临时环境变量：直接输入以下命令\n",
    "```js\n",
    "// 查看当前文件地址\n",
    "pwd\n",
    "// 设置临时环境变量\n",
    "export PATH=$PATH:/home/wangjunjun/ollama/bin\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d36cde",
   "metadata": {},
   "source": [
    "下载模型，在官网选择\n",
    "```js\n",
    "// #设置环境变量\n",
    "ollama run qwen2\n",
    "// #没有设置环境变量\n",
    "./ollama run qwen2\n",
    "````\n",
    "可以用 `ollama list` 查看已经下载的模型列表\n",
    "```js\n",
    "// #设置环境变量\n",
    "ollama list\n",
    "// #没有设置环境变量\n",
    "./ollama list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daab823",
   "metadata": {},
   "source": [
    "# LangChain + Ollama 下的RAG\n",
    "选用 ChatOllama：https://python.langchain.com/docs/integrations/chat/ollama/\n",
    "\n",
    "注意要先安装Ollima `pip install -U ollama`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21badf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入包\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "model_dir = './qwen/Qwen-14B-Chat'  # 模型的路径\n",
    "model = ChatOllama(model=model_dir, temperature=0)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
