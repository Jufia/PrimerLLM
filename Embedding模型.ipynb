{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddcf5342",
   "metadata": {},
   "source": [
    "# RAG 流程\n",
    "RAG的逻辑是先将本地库向量化，生成一个向量库，导入到大语言模型中，再将问题向量化，与向量库中的数据对比，输出答案。\n",
    "\n",
    "需要用到：\n",
    "- Embedding模型\n",
    "- 向量数据库\n",
    "- 大模型\n",
    "\n",
    "使用 LangChain 库来搭建，参考文章：https://zhuanlan.zhihu.com/p/668082024\n",
    "\n",
    "## 简要介绍 LangChain 库：\n",
    "介绍来自于：https://github.com/liaokongVFX/LangChain-Chinese-Getting-Started-Guide  \n",
    "\n",
    "LangChain是一个软件开发框架，可以更轻松地使用大型语言模型（LLM）创建应用程序。它是一个具有 Python 和 JavaScript 代码库的开源工具。LangChain 允许开发人员将 GPT-4 等 LLM 与外部数据相结合，为聊天机器人、代码理解、摘要等各种应用程序开辟了可能性。\n",
    "\n",
    "LangChain 有几个组件（https://python.langchain.com/docs/integrations/components/）：\n",
    "- Document loaders 对索引的支持\n",
    "  - 加载数据, 拥有大量的文档加载器，比如 Email、Markdown、PDF、Youtube ... `langchain.document_loaders`用于导入.txt文件\n",
    "  - 文档分割器 `langchain.text_splitter.CharacterTextSplitter`\n",
    "- Embedding models\n",
    "  - 向量化 `langchain.embeddings.HuggingFaceBgeEmbeddings`\n",
    "- Vector stores\n",
    "  - 向量库，对接向量存储与搜索，比如 Chroma、Pinecone、Qdrand `langchain.vectorstores.Chroma`\n",
    "- Chat models 大语言模型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b057a5f8",
   "metadata": {},
   "source": [
    "# 本地数据加载\n",
    "使用 Loader 加载器，可以加载PDF，web，CSV，directory，HTIL，JSON等文件类型，官方：https://python.langchain.com/docs/how_to/#document-loaders  \n",
    "\n",
    "当使用loader加载器读取到数据源后，数据源需要转换成 Document 对象后，后续才能进行使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71c01b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader  \n",
    "\n",
    "loader = TextLoader('./data/jd.txt')\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7340fe6",
   "metadata": {},
   "source": [
    "# Text Spltters 文本分割\n",
    "为什么需要分割文本？因为我们每次不管是做把文本当作 prompt 发给 openai api ，还是还是使用 openai api embedding 功能都是有字符限制的。比如我们将一份300页的 pdf 发给 openai api，让他进行总结，他肯定会报超过最大 Token 错。所以这里就需要使用文本分割器去分割我们 loader 进来的 Document。\n",
    "\n",
    "借助langchain的字符分割器, 假设采用固定字符长度分割chunk_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f5dd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# 创建拆分器\n",
    "\"\"\"\n",
    "参数介绍\n",
    "- separators - 分隔符字符串数组\n",
    "- chunk_size - 每个文档的字符数量限制\n",
    "- chunk_overlap - 两份文档重叠区域的长度\n",
    "- length_function - 长度计算函数\n",
    "- is_separator_regex - 如果为真：应当被解释为正则表达式，因此不需要转义。如果为假：应当被当作普通字符串分隔符，并转义任何特殊字符。\n",
    "\"\"\"\n",
    "text_spliter = CharacterTextSplitter(\n",
    "    separator='\\n\\n',\n",
    "    chunk_size = 128, \n",
    "    chunk_overlap = 20,\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "# 拆分文档，文档被拆分成n份，例如 {metadata 1}{metadata 2}{metadata 3}\n",
    "documents = text_spliter.split_documents(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e195b48",
   "metadata": {},
   "source": [
    "# 向量化&数据入库\n",
    "这一步是要建立向量库，因为数据相关性搜索其实是向量运算。所以，不管我们是使用 openai api embedding 功能还是直接通过向量数据库直接查询，都需要将我们的加载进来的数据 Document 进行向量化，才能进行向量运算搜索。转换成向量也很简单，只需要我们把数据存储到对应的向量数据库中即可完成向量的转换。  \n",
    "\n",
    "这里选用 m3e-base作为embedding模型，用 Chroma 建立向量库\n",
    "\n",
    "官方Embedding模型：https://python.langchain.com/docs/integrations/text_embedding/\n",
    "官方向量库：https://python.langchain.com/docs/integrations/vectorstores/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98b08f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一次导入会自动下载，可能会有网络问题，在开头添加以下两句\n",
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# 向量化&数据入库\n",
    "embedding = HuggingFaceBgeEmbeddings(\n",
    "    model_name = \"moka-ai/m3e-base\",\n",
    "    model_kwargs = {'device': 'cpu'},\n",
    "    encode_kwargs = {'normalize_embeddings': True},\n",
    "    query_instruction=\"为文本生成向量表示用于文本检索\",\n",
    ")\n",
    "\n",
    "# load data to Chroma 向量数据库\n",
    "db = Chroma.from_documents(documents, embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f732067",
   "metadata": {},
   "source": [
    "# 查询\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61534e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.similarity_search('这是什么岗位？')\n",
    "# 输出文档按照相关性大小排序 n 份结果，例如{metadata 1}{metadata 3}{metadata 2}\n",
    "# 此时没有导入大语言模型，不会得到归纳后的答案"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d890bfa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
